{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming y tokenización con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cperales/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/cperales/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cperales/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "from string import punctuation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracterización del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias del corpus = ['neg', 'pos']\n",
      "Total de reviews = 2000\n",
      "Cantidad de reviews positivas = 1000\n",
      "Cantidad de reviews negativas = 1000\n"
     ]
    }
   ],
   "source": [
    "print('Categorias del corpus =', movie_reviews.categories())\n",
    "print('Total de reviews =', len(movie_reviews.fileids()))\n",
    "print('Cantidad de reviews positivas =', len(movie_reviews.fileids('pos')))\n",
    "print('Cantidad de reviews negativas =', len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos filtrar el corpus, aplicando stopwords y stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords en inglés = <WordListCorpusReader in '/home/cperales/nltk_data/corpora/stopwords'>\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "print('Stopwords en inglés =', stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(cat, final_label = None):\n",
    "    if final_label is None:\n",
    "        final_label = cat\n",
    "    words = []\n",
    "    ids = movie_reviews.fileids(cat)\n",
    "    for id_ in ids:\n",
    "        for word in movie_reviews.words(id_):\n",
    "            if word not in stopwords_english and word not in punctuation:\n",
    "                words.append(({'word': english_stemmer.stem(word.lower())}, final_label))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtramos el corpus aplicando stopwords en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = feature_generator('pos', 1)\n",
    "neg_words = feature_generator('neg', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el dataset, y lo dividimos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El set completo tiene 710578 elementos\n",
      "El set de test tiene 355289 elementos\n",
      "El set de train tiene 355289 elementos\n"
     ]
    }
   ],
   "source": [
    "whole_set = pos_words + neg_words\n",
    "shuffle(whole_set)\n",
    "print('El set completo tiene', len(whole_set), 'elementos')\n",
    "prop = int(0.5 * len(whole_set))\n",
    "train_set, test_set = whole_set[:prop], whole_set[prop:]\n",
    "print('El set de test tiene', len(test_set), 'elementos')\n",
    "print('El set de train tiene', len(train_set), 'elementos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos clasificar y testear el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del método es 0.5697446304276237\n",
      "La precisión del método es 0.6107872745849153\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print('La precisión del método es', accuracy(classifier, test_set))\n",
    "print('La precisión del método es', accuracy(classifier, train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    word = 'mulan'             1 : 0      =     31.3 : 1.0\n",
      "                    word = 'bulworth'          1 : 0      =     19.4 : 1.0\n",
      "                    word = 'prinz'             0 : 1      =     17.5 : 1.0\n",
      "                    word = 'freddi'            0 : 1      =     17.5 : 1.0\n",
      "                    word = 'garbag'            0 : 1      =     16.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(text):\n",
    "    words = [{'word': english_stemmer.stem(word.lower())} for word in word_tokenize(' '.join(text.split()))\n",
    "             if word not in stopwords_english and word not in punctuation]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'i'}, {'word': 'hate'}, {'word': 'film'}, {'word': 'it'}, {'word': 'disast'}, {'word': 'it'}, {'word': 'poor'}, {'word': 'direct'}, {'word': 'bad'}, {'word': 'act'}]\n"
     ]
    }
   ],
   "source": [
    "custom_review = \"I hated the film. It was a disaster. It has poor direction and bad acting.\"\n",
    "custom_review_tokens = feature_extractor(custom_review)\n",
    "print(custom_review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"localhost\", 27017)  # Qué significa localhost\n",
    "db = client.tweets\n",
    "collection = db.movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = [tweet['text'] for tweet in collection.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahogany Teakwood candle lit (my absolute favorite scent), Fantastic Beasts on, wine poured. After the past few cra… https://t.co/dHKac5WPw5\n"
     ]
    }
   ],
   "source": [
    "tweet = tweet_text[5]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'mahogani'}, {'word': 'teakwood'}, {'word': 'candl'}, {'word': 'lit'}, {'word': 'absolut'}, {'word': 'favorit'}, {'word': 'scent'}, {'word': 'fantast'}, {'word': 'beast'}, {'word': 'wine'}, {'word': 'pour'}, {'word': 'after'}, {'word': 'past'}, {'word': 'cra…'}, {'word': 'https'}, {'word': '//t.co/dhkac5wpw5'}]\n"
     ]
    }
   ],
   "source": [
    "features_tweet = feature_extractor(tweet)\n",
    "print(features_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(classifier.classify_many(features_tweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
